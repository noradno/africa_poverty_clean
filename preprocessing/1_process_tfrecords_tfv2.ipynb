{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Pre-requisites\" data-toc-modified-id=\"Pre-requisites-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Pre-requisites</a></span></li><li><span><a href=\"#Instructions\" data-toc-modified-id=\"Instructions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Instructions</a></span></li><li><span><a href=\"#Imports-and-Constants\" data-toc-modified-id=\"Imports-and-Constants-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports and Constants</a></span></li><li><span><a href=\"#Validate-and-Split-Exported-TFRecords\" data-toc-modified-id=\"Validate-and-Split-Exported-TFRecords-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Validate and Split Exported TFRecords</a></span></li><li><span><a href=\"#Calculate-Mean-and-Std-Dev-for-Each-Band\" data-toc-modified-id=\"Calculate-Mean-and-Std-Dev-for-Each-Band-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Calculate Mean and Std-Dev for Each Band</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Go through the [`preprocessing/0_export_tfrecords.ipynb`](./0_export_tfrecords.ipynb) notebook.\n",
    "\n",
    "Before running this notebook, you should have the following structure under the `data/` directory:\n",
    "\n",
    "```\n",
    "data/\n",
    "    dhs_tfrecords_raw/\n",
    "        angola_2011_00.tfrecord.gz\n",
    "        ...\n",
    "        zimbabwe_2015_XX.tfrecord.gz\n",
    "    dhsnl_tfrecords_raw/\n",
    "        angola_2010_00.tfrecord.gz\n",
    "        ...\n",
    "        zimbabwe_2016_XX.tfrecord.gz\n",
    "    lsms_tfrecords_raw/\n",
    "        ethiopia_2011_00.tfrecord.gz\n",
    "        ...\n",
    "        uganda_2013_XX.tfrecord.gz\n",
    "```\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook processes the exported TFRecords as follows:\n",
    "1. Verifies that the fields in the TFRecords match the original CSV files.\n",
    "2. Splits each monolithic TFRecord file exported from Google Earth Engine into one file per record.\n",
    "\n",
    "After running this notebook, you should have three new folders (`dhs_tfrecords`, `dhsnl_tfrecords`, and `lsms_tfrecords`) under `data/`:\n",
    "\n",
    "```\n",
    "data/\n",
    "    dhs_tfrecords/\n",
    "        angola_2011/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            00229.tfrecord.gz\n",
    "        ...\n",
    "        zimbabwe_2015/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            00399.tfrecord.gz\n",
    "    dhsnl_tfrecords/\n",
    "        angola_2010/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            07734.tfrecord.gz\n",
    "        zimbabwe_2016/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            03584.tfrecord.gz\n",
    "    lsms_tfrecords/\n",
    "        ethiopia_2011/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            00326.tfrecord.gz\n",
    "        uganda_2013/\n",
    "            00000.tfrecord.gz\n",
    "            ...\n",
    "            00164.tfrecord.gz\n",
    "```\n",
    "\n",
    "This notebook also calculates the mean and standard deviation of each band across each of the 3 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change directory to repo root, and verify\n",
    "%cd '~/Github/africa_poverty_clean'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from batchers import batcher, tfrecord_paths_utils\n",
    "from preprocessing.helper import (\n",
    "    analyze_tfrecord_batch,\n",
    "    per_band_mean_std,\n",
    "    print_analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_BANDS = [\n",
    "    'BLUE', 'GREEN', 'LAT', 'LON', 'NIGHTLIGHTS', 'NIR', 'RED',\n",
    "    'SWIR1', 'SWIR2', 'TEMP1']\n",
    "\n",
    "BANDS_ORDER = [\n",
    "    'BLUE', 'GREEN', 'RED', 'SWIR1', 'SWIR2', 'TEMP1', 'NIR',\n",
    "    'DMSP', 'VIIRS']\n",
    "\n",
    "DHS_EXPORT_FOLDER = 'data/dhs_tfrecords_raw'\n",
    "DHSNL_EXPORT_FOLDER = 'data/dhsnl_tfrecords_raw'\n",
    "LSMS_EXPORT_FOLDER = 'data/lsms_tfrecords_raw'\n",
    "\n",
    "DHS_PROCESSED_FOLDER = 'data/dhs_tfrecords'\n",
    "DHSNL_PROCESSED_FOLDER = 'data/dhsnl_tfrecords'\n",
    "LSMS_PROCESSED_FOLDER = 'data/lsms_tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate and Split Exported TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(csv_path: str, input_dir: str, processed_dir: str) -> None:\n",
    "    '''\n",
    "    Args\n",
    "    - csv_path: str, path to CSV of DHS or LSMS clusters\n",
    "    - input_dir: str, path to TFRecords exported from Google Earth Engine\n",
    "    - processed_dir: str, folder where to save processed TFRecords\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path, float_precision='high', index_col=False)\n",
    "    surveys = list(df.groupby(['country', 'year']).groups.keys())  # (country, year) tuples\n",
    "\n",
    "    for country, year in surveys:\n",
    "        country_year = f'{country}_{year}'\n",
    "        print('Processing:', country_year)\n",
    "\n",
    "        tfrecord_paths = glob(os.path.join(input_dir, country_year + '*'))\n",
    "        out_dir = os.path.join(processed_dir, country_year)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        subset_df = df[(df['country'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
    "        validate_and_split_tfrecords(\n",
    "            tfrecord_paths=tfrecord_paths, out_dir=out_dir, df=subset_df)\n",
    "\n",
    "\n",
    "def validate_and_split_tfrecords(\n",
    "        tfrecord_paths: Iterable[str],\n",
    "        out_dir: str,\n",
    "        df: pd.DataFrame\n",
    "        ) -> None:\n",
    "    '''Validates and splits a list of exported TFRecord files (for a\n",
    "    given country-year survey) into individual TFrecords, one per cluster.\n",
    "\n",
    "    \"Validating\" a TFRecord comprises of 2 parts\n",
    "    1) verifying that it contains the required bands\n",
    "    2) verifying that its other features match the values from the dataset CSV\n",
    "\n",
    "    Args\n",
    "    - tfrecord_paths: list of str, paths to exported TFRecords files\n",
    "    - out_dir: str, path to dir to save processed individual TFRecords\n",
    "    - df: pd.DataFrame, index is sequential and starts at 0\n",
    "    '''\n",
    "    # Create an iterator over the TFRecords file. The iterator yields\n",
    "    # the binary representations of Example messages as strings.\n",
    "    #options = tf.io.TFRecordOptions(compression_type = 'GZIP')\n",
    "\n",
    "    # cast float64 => float32 and str => bytes\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        elif df[col].dtype == object:  # pandas uses 'object' type for str\n",
    "            df[col] = df[col].astype(bytes)\n",
    "\n",
    "    i = 0\n",
    "    progbar = tqdm(total=len(df))\n",
    "\n",
    "    for tfrecord_path in tfrecord_paths:\n",
    "        iterator = tf.data.TFRecordDataset(tfrecord_path, compression_type='GZIP')\n",
    "        for record_str in iterator.take:\n",
    "            # parse into an actual Example message\n",
    "\n",
    "            ex = tf.train.Example.FromString(record_str)\n",
    "            feature_map = ex.features.feature\n",
    "\n",
    "            # verify required bands exist\n",
    "            for band in REQUIRED_BANDS:\n",
    "                assert band in feature_map, f'Band \"{band}\" not in record {i} of {tfrecord_path}'\n",
    "\n",
    "            # compare feature map values against CSV values\n",
    "            csv_feats = df.loc[i, :].to_dict()\n",
    "            for col, val in csv_feats.items():\n",
    "                ft_type = feature_map[col].WhichOneof('kind')\n",
    "                ex_val = feature_map[col].__getattribute__(ft_type).value[0]\n",
    "                assert val == ex_val, f'Expected {col}={val}, but found {ex_val} instead'\n",
    "\n",
    "            # serialize to string and write to file\n",
    "            out_path = os.path.join(out_dir, f'{i:05d}.tfrecord.gz')  # all surveys have < 1e6 clusters\n",
    "            with tf.io.TFRecordWriter(out_path, options=options) as writer:\n",
    "                writer.write(ex.SerializeToString())\n",
    "\n",
    "            i += 1\n",
    "            progbar.update(1)\n",
    "    progbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: angola_2011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812555b608654a479bddb88df62cbcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/qhh9f94576qdllgxmdkpmm000000gn/T/ipykernel_93349/3832109671.py:59: RuntimeWarning: Unexpected end-group tag: Not all data was converted\n",
      "  ex = tf.train.Example.FromString(record_str)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Band \"BLUE\" not in record 0 of data/dhs_tfrecords_raw/angola_2011_01.tfrecord.gz",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/dhs_clusters.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDHS_EXPORT_FOLDER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDHS_PROCESSED_FOLDER\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(csv_path, input_dir, processed_dir)\u001b[0m\n\u001b[1;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(out_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m subset_df \u001b[38;5;241m=\u001b[39m df[(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m country) \u001b[38;5;241m&\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m year)]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mvalidate_and_split_tfrecords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfrecord_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfrecord_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 64\u001b[0m, in \u001b[0;36mvalidate_and_split_tfrecords\u001b[0;34m(tfrecord_paths, out_dir, df)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# verify required bands exist\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m band \u001b[38;5;129;01min\u001b[39;00m REQUIRED_BANDS:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m band \u001b[38;5;129;01min\u001b[39;00m feature_map, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBand \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mband\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not in record \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtfrecord_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# compare feature map values against CSV values\u001b[39;00m\n\u001b[1;32m     67\u001b[0m csv_feats \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[i, :]\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "\u001b[0;31mAssertionError\u001b[0m: Band \"BLUE\" not in record 0 of data/dhs_tfrecords_raw/angola_2011_01.tfrecord.gz"
     ]
    }
   ],
   "source": [
    "process_dataset(\n",
    "    csv_path='data/dhs_clusters.csv',\n",
    "    input_dir=DHS_EXPORT_FOLDER,\n",
    "    processed_dir=DHS_PROCESSED_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/c2/qhh9f94576qdllgxmdkpmm000000gn/T/ipykernel_93349/3832109671.py\u001b[0m(64)\u001b[0;36mvalidate_and_split_tfrecords\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     62 \u001b[0;31m            \u001b[0;31m# verify required bands exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     63 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mband\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mREQUIRED_BANDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 64 \u001b[0;31m                \u001b[0;32massert\u001b[0m \u001b[0mband\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'Band \"{band}\" not in record {i} of {tfrecord_path}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     65 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     66 \u001b[0;31m            \u001b[0;31m# compare feature map values against CSV values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ex\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_dataset(\n",
    "#    csv_path='data/dhsnl_locs.csv',\n",
    "#    input_dir=DHSNL_EXPORT_FOLDER,\n",
    "#    processed_dir=DHSNL_PROCESSED_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    csv_path='data/lsms_clusters.csv',\n",
    "    input_dir=LSMS_EXPORT_FOLDER,\n",
    "    processed_dir=LSMS_PROCESSED_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the Individual TFRecord Files (Optional)\n",
    "\n",
    "Check that the label, location, and year values in each individual TFRecord file match the original CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_individual_tfrecords(tfrecord_paths: Iterable[str],\n",
    "                                  csv_path: str,\n",
    "                                  label_name: Optional[str] = None) -> None:\n",
    "    '''\n",
    "    Args\n",
    "    - tfrecord_paths: list of str, paths to individual TFRecord files\n",
    "        in the same order as in the CSV\n",
    "    - csv_path: str, path to CSV file with columns ['lat', 'lon', 'wealthpooled', 'year']\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path, float_precision='high', index_col=False)\n",
    "    iter_init, batch_op = batcher.Batcher(\n",
    "        tfrecord_files=tfrecord_paths,\n",
    "        label_name=label_name,\n",
    "        ls_bands=None,\n",
    "        nl_band=None,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        augment=False,\n",
    "        clipneg=False,\n",
    "        normalize=None).get_batch()\n",
    "\n",
    "    locs, years = [], []\n",
    "    if label_name is not None:\n",
    "        labels = []\n",
    "\n",
    "    num_processed = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iter_init)\n",
    "        while True:\n",
    "            try:\n",
    "                if label_name is not None:\n",
    "                    batch_np = sess.run((batch_op['locs'], batch_op['years'], batch_op['labels']))\n",
    "                    labels.append(batch_np[2])\n",
    "                else:\n",
    "                    batch_np = sess.run((batch_op['locs'], batch_op['years']))\n",
    "                locs.append(batch_np[0])\n",
    "                years.append(batch_np[1])\n",
    "                num_processed += len(batch_np[0])\n",
    "                print(f'\\rProcessed {num_processed} images', end='')\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "    print()\n",
    "\n",
    "    locs = np.concatenate(locs)\n",
    "    years = np.concatenate(years)\n",
    "    assert (locs == df[['lat', 'lon']].to_numpy(dtype=np.float32)).all()\n",
    "    assert (years == df['year'].to_numpy(dtype=np.float32)).all()\n",
    "    if label_name is not None:\n",
    "        labels = np.concatenate(labels)\n",
    "        assert (labels == df['wealthpooled'].to_numpy(dtype=np.float32)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_individual_tfrecords(\n",
    "    tfrecord_paths=tfrecord_paths_utils.dhs(),\n",
    "    csv_path='data/dhs_clusters.csv',\n",
    "    label_name='wealthpooled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_individual_tfrecords(\n",
    "    tfrecord_paths=tfrecord_paths_utils.dhsnl(),\n",
    "    csv_path='data/dhsnl_locs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_individual_tfrecords(\n",
    "    tfrecord_paths=tfrecord_paths_utils.lsms(),\n",
    "    csv_path='data/lsms_clusters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Mean and Std-Dev for Each Band\n",
    "\n",
    "The means and standard deviations calculated here are saved as constants in `batchers/dataset_constants.py` for `_MEANS_DHS`, `_STD_DEVS_DHS`, `_MEANS_LSMS`, and `_STD_DEVS_LSMS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(tfrecord_paths):\n",
    "    '''Calculates and prints the per-band means and std-devs'''\n",
    "    iter_init, batch_op = batcher.Batcher(\n",
    "        tfrecord_files=tfrecord_paths,\n",
    "        label_name=None,\n",
    "        ls_bands='ms',\n",
    "        nl_band='merge',\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        augment=False,\n",
    "        clipneg=False,\n",
    "        normalize=None).get_batch()\n",
    "\n",
    "    stats = analyze_tfrecord_batch(\n",
    "        iter_init, batch_op, total_num_images=len(tfrecord_paths),\n",
    "        nbands=len(BANDS_ORDER), k=10)\n",
    "    means, stds = per_band_mean_std(stats=stats, band_order=BANDS_ORDER)\n",
    "\n",
    "    print('Means:')\n",
    "    pprint(means)\n",
    "    print()\n",
    "\n",
    "    print('Std Devs:')\n",
    "    pprint(stds)\n",
    "\n",
    "    print('\\n========== Additional Per-band Statistics ==========\\n')\n",
    "    print_analysis_results(stats, BANDS_ORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mean_std(tfrecord_paths_utils.dhs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mean_std(tfrecord_paths_utils.dhsnl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_mean_std(tfrecord_paths_utils.lsms())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:r3py3] *",
   "language": "python",
   "name": "conda-env-r3py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
